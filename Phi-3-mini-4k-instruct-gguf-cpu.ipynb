{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6eecbd-f197-4550-a87e-4af74eee3218",
   "metadata": {},
   "source": [
    "# Simple inference Phi-3-mini-4k-instruct-gguf with CPU\n",
    "This notebook shows how to use `Phi-3-mini-4k-instruct-gguf` for basic inference\n",
    "\n",
    "More information is available at [HuggingFace Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a3cbc-3232-46f2-a88e-b118e9b2d53e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CPU specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750023ba-9cc1-4f18-9080-a51f9f8a8043",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Address sizes:                      48 bits physical, 48 bits virtual\n",
      "Byte Order:                         Little Endian\n",
      "CPU(s):                             24\n",
      "On-line CPU(s) list:                0-23\n",
      "Vendor ID:                          AuthenticAMD\n",
      "Model name:                         AMD Ryzen 9 7900X 12-Core Processor\n",
      "CPU family:                         25\n",
      "Model:                              97\n"
     ]
    }
   ],
   "source": [
    "! lscpu | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3fe42dd-c72e-4807-abb5-693625ccc0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Set the number of cores to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2859f8b8-96aa-4f18-adc0-8b8f088ba806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CPU_CORES = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78e281f-2256-4c07-8e39-926fa4a26d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "model_shard_dir='/home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf'\n",
    "model_shard_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7dda188-a703-4941-9058-f0643e1fb59e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(model_shard_dir\n",
    "              , ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "243c4d70-394e-41d9-bed7-f06b772befb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b762644-0630-491e-86d4-984eea45d85d",
   "metadata": {},
   "source": [
    "# Clone the git repo\n",
    "First get the git repo but without cloning the LFS objects using `GIT_LFS_SKIP_SMUDGE=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9863d679-d73f-41b3-a924-df922811d49c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'model-cache/Phi-3-mini-4k-instruct-gguf'...\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (15/15), 10.58 KiB | 10.58 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf model-cache/Phi-3-mini-4k-instruct-gguf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f0907-72e5-4459-bf57-6c73daff6770",
   "metadata": {},
   "source": [
    "Examine the artifacts\n",
    "\n",
    "Notice the gguf file are only 136 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ebd8e1-cde9-48b1-8eaf-b2250888c904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48K\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.8K May 22 17:09 NOTICE.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  251 May 22 17:09 Modelfile_q4\n",
      "-rw-r--r-- 1 ubuntu ubuntu  253 May 22 17:09 Modelfile_fp16\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.1K May 22 17:09 LICENSE\n",
      "-rw-r--r-- 1 ubuntu ubuntu  444 May 22 17:09 CODE_OF_CONDUCT.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.6K May 22 17:09 SECURITY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  14K May 22 17:09 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  135 May 22 17:09 Phi-3-mini-4k-instruct-q4.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu  135 May 22 17:09 Phi-3-mini-4k-instruct-fp16.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/Phi-3-mini-4k-instruct-gguf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274a055-7512-474a-9d5d-4648680d3fb3",
   "metadata": {},
   "source": [
    "## Download the model Phi-3-mini-4k-instruct-q4.gguf\n",
    "First we test the __Phi-3-mini-4k-instruct-q4.gguf__ model\n",
    "\n",
    "The model card lists this as `medium, balanced quality - recommended`\n",
    "\n",
    "Now download the artifact Phi-3-mini-4k-instruct-q4.gguf\n",
    "\n",
    "### Model aspects\n",
    "1. File: Phi-3-mini-4k-instruct-q4.gguf\n",
    "2. Quantization method: Q4_K_M\t\n",
    "3. Bits: 4\n",
    "4. File size: 2.2GB\n",
    "5. Memory required: 4.7 GB\t\n",
    "6. Use case: medium, balanced quality - recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4376907b-cc5c-4447-9749-5c985bef1cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFS objects: 100% (1/1), 2.4 GB | 109 MB/s                          \r"
     ]
    }
   ],
   "source": [
    "! cd model-cache/Phi-3-mini-4k-instruct-gguf  && git-lfs pull --include=Phi-3-mini-4k-instruct-q4.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26568b9e-078b-4583-a983-b739be5522f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.3G\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.8K May 22 17:09 NOTICE.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  251 May 22 17:09 Modelfile_q4\n",
      "-rw-r--r-- 1 ubuntu ubuntu  253 May 22 17:09 Modelfile_fp16\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.1K May 22 17:09 LICENSE\n",
      "-rw-r--r-- 1 ubuntu ubuntu  444 May 22 17:09 CODE_OF_CONDUCT.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.6K May 22 17:09 SECURITY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  14K May 22 17:09 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  135 May 22 17:09 Phi-3-mini-4k-instruct-fp16.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.3G May 22 17:09 Phi-3-mini-4k-instruct-q4.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/Phi-3-mini-4k-instruct-gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d8aa9-98b3-4e2d-aa7e-88932d738606",
   "metadata": {},
   "source": [
    "## Model inferencing with Phi-3-mini-4k-instruct-q4.gguf\n",
    "`Phi-3-mini-4k-instruct-gguf` requires the `llama-cpp-python` to be installed\n",
    "\n",
    "You can do this with\n",
    "```bash\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f58e50-032d-40aa-b37d-13009424cc36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2281.66 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   340.56 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    20.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 260\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=f\"{model_shard_dir}/Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\n",
    "  n_ctx=4096,     # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=CPU_CORES,    # The number of CPU threads to use\n",
    "  n_gpu_layers=0, # Set to 0 to disable GPU offload, model will infer on CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb67a43-6b99-4ed7-b415-b1d7c398de9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            61Gi       7.5Gi        28Gi       1.6Gi        25Gi        52Gi\n",
      "Swap:          8.0Gi          0B       8.0Gi\n"
     ]
    }
   ],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2227c3-6b1f-4fa4-86da-96265ab1cf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     119.17 ms\n",
      "llama_print_timings:      sample time =      27.23 ms /   256 runs   (    0.11 ms per token,  9402.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     119.08 ms /    12 tokens (    9.92 ms per token,   100.78 tokens per second)\n",
      "llama_print_timings:        eval time =   12511.34 ms /   255 runs   (   49.06 ms per token,    20.38 tokens per second)\n",
      "llama_print_timings:       total time =   12975.24 ms /   267 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is an embedding model?<|end|>\n",
      "<|assistant|> An embedding model, in the context of machine learning and natural language processing (NLP), refers to a mathematical representation where words, phrases, sentences, or even entire documents are mapped to vectors of real numbers. Each unique word or phrase from the vocabulary gets associated with one vector in this high-dimensional space. The primary purpose of embedding models is to capture semantic meaning and contextual relationships between terms in such a way that similar meanings have similar representations.\n",
      "\n",
      "There are various types of embedding models, but some of the most popular include:\n",
      "\n",
      "1. Word Embeddings (e.g., Word2Vec, GloVe): These models focus on representing individual words. They learn to predict a word given its context or vice versa and produce dense vector representations for each unique word in the training corpus.\n",
      "\n",
      "2. Sentence/Document Embeddings: Unlike word embeddings which are limited to single tokens, sentence and document embedding models can capture semantic relationships between entire phrases or sentences. Examples of such models include Doc2Vec (or Paragraph Vector), BERT (Bidirectional Encoder Representations from Transformers), and Sentence-BERT.\n",
      "\n",
      "3. Know\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is an embedding model?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9edf10-09fe-4647-b906-9cd7a5fed353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     119.17 ms\n",
      "llama_print_timings:      sample time =      26.84 ms /   256 runs   (    0.10 ms per token,  9539.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   12494.35 ms /   256 runs   (   48.81 ms per token,    20.49 tokens per second)\n",
      "llama_print_timings:       total time =   12865.25 ms /   256 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What are closures in functional programming?<|end|>\n",
      "<|assistant|> Closures are a fundamental concept in functional programming and computer science at large. A closure is an expression that captures the lexical bindings of free variables. In other words, it's a function combined with a referencing environment—this environment comprises any non-local variables used by the function. When you create a function within another function (nested), and if this inner function references variables from its outer scope(s) that are not parameters to the inner function, then the outer function is said to have 'closed over' these variables, thus creating a closure.\n",
      "\n",
      "\n",
      "### Key Characteristics of Closures:\n",
      "\n",
      "1. **Environment Capture**: The enclosing function captures any local variables from its scope at the time of its creation. This means that when you return this inner function as a value or pass it to another part of your program, these captured variables are still accessible and retain their values even after the outer function has finished execution.\n",
      "\n",
      "2. **Hoisting Behavior**: In many programming languages like JavaScript (and others), closures can exhibit \"hoisting\" behavior where variable declarations outside a block are moved to the top of the scope during compilation. However, it's important to\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are closures in functional programming?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b74c4-961e-4960-a0a6-736e68ec37f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the model Phi-3-mini-4k-instruct-fp16.gguf\n",
    "Next we test the __Phi-3-mini-4k-instruct-fp16.gguf__ model\n",
    "\n",
    "The model card lists this as `minimal quality loss`\n",
    "\n",
    "Now download the artifact [Phi-3-mini-4k-instruct-q4.gguf](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-fp16.gguf)\n",
    "\n",
    "### Model aspects\n",
    "1. File: Phi-3-mini-4k-instruct-q4.gguf\n",
    "2. Quantization method: None\t\n",
    "3. Bits: NA\n",
    "4. File size: 7.2 GB\n",
    "5. Memory required: 9.3 GiB\n",
    "6. Use case: minimal quality loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9374667b-5426-4e2d-bd0f-2446bab9be4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFS objects: 100% (1/1), 7.6 GB | 111 MB/s                          \r"
     ]
    }
   ],
   "source": [
    "! cd model-cache/Phi-3-mini-4k-instruct-gguf  && git-lfs pull --include=Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2555392-cf29-4dbc-82ce-0f8515dd46e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9.4G\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.8K May 22 17:09 NOTICE.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  251 May 22 17:09 Modelfile_q4\n",
      "-rw-r--r-- 1 ubuntu ubuntu  253 May 22 17:09 Modelfile_fp16\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.1K May 22 17:09 LICENSE\n",
      "-rw-r--r-- 1 ubuntu ubuntu  444 May 22 17:09 CODE_OF_CONDUCT.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.6K May 22 17:09 SECURITY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  14K May 22 17:09 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.3G May 22 17:09 Phi-3-mini-4k-instruct-q4.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu 7.2G May 22 17:16 Phi-3-mini-4k-instruct-fp16.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/Phi-3-mini-4k-instruct-gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a8a39-554e-45d7-9e2f-5c96830c9752",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model inferencing with Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e9bb51-f295-4c07-8111-277d3a41486c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  130 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7288.51 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   370.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    20.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 260\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=f\"{model_shard_dir}/Phi-3-mini-4k-instruct-fp16.gguf\",  # path to GGUF file\n",
    "  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=CPU_CORES, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=0, # Disable GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "692dfdf7-27ff-461c-853c-5579741f03d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            61Gi       7.5Gi        28Gi       1.6Gi        25Gi        52Gi\n",
      "Swap:          8.0Gi          0B       8.0Gi\n"
     ]
    }
   ],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0d3dbd-052a-49f6-b913-89b8e7bd8d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     229.30 ms\n",
      "llama_print_timings:      sample time =      17.72 ms /   168 runs   (    0.11 ms per token,  9481.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     228.98 ms /    12 tokens (   19.08 ms per token,    52.41 tokens per second)\n",
      "llama_print_timings:        eval time =   23630.10 ms /   167 runs   (  141.50 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =   24105.49 ms /   179 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is an embedding model?<|end|>\n",
      "<|assistant|> An embedding model is a representation technique in machine learning and natural language processing (NLP) that converts categorical data, such as words or phrases, into continuous vectors of real numbers. These numerical representations capture the semantic relationships between items, enabling algorithms to process and analyze textual information more effectively. Embeddings are learned from data by mapping similar elements closer together in the embedding space and dissimilar ones farther apart. This transformation often improves performance on various tasks such as sentiment analysis, language translation, or recommendation systems. Popular examples of embedding models include Word2Vec, GloVe, and fastText for words and sentences, as well as sentence-BERT and transformer-based architectures like BERT (Bidirectional Encoder Representations from Transformers) for more complex NLP applications.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is an embedding model?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e22cd9b-fec8-4ef4-acbc-d01523d88e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     229.30 ms\n",
      "llama_print_timings:      sample time =      26.81 ms /   256 runs   (    0.10 ms per token,  9549.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     161.86 ms /    10 tokens (   16.19 ms per token,    61.78 tokens per second)\n",
      "llama_print_timings:        eval time =   36061.15 ms /   255 runs   (  141.42 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =   36602.09 ms /   265 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What are closures in functional programming?<|end|>\n",
      "<|assistant|> Closures, in the context of functional programming, refer to a powerful concept where a function has access to its surrounding lexical scope even after the outer function has finished executing. This means that functions can capture and retain references to variables from their defining environments. Here's how closures work and why they are important:\n",
      "\n",
      "1. Functional Encapsulation: Closures allow you to encapsulate state within a function, which helps in creating private data or methods. The captured variables (also called free variables) can be accessed by the enclosed function even when it's executed outside of its defining scope. This enables functional programming techniques like higher-order functions, where functions are treated as first-class citizens and passed around as arguments to other functions.\n",
      "\n",
      "2. Functional Abstraction: Closures allow for abstractions in functional programming by hiding the implementation details while exposing an interface through function expressions or anonymous functions (lambdas). These closures can capture values that aren't explicitly returned, allowing you to define complex behavior without relying on global variables and state.\n",
      "\n",
      "3. Data Immutability: By using closures for encapsulation of data, functional programming languages help enforce immutability—a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are closures in functional programming?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2e474-b090-4e76-a3f5-a3ee27a1e587",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Free the GPU memory\n",
    "Reset the cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5743a1e9-651e-42c5-a921-705c925440c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ce66ba-f279-4a5d-8e4b-26226a35d079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 17:19:48 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   36C    P0             63W /  450W |       7MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964f2c2-c87c-46fa-9d45-3938b7801c64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean up the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4f35a-007b-42b9-910a-bf92facbc53a",
   "metadata": {},
   "source": [
    "Next remove the downloaded artifacts to free disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f25d0e-c5cf-42c7-9070-af4dfb78fc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf model-cache/CodeLlama-7B-GGUF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 for LLM",
   "language": "python",
   "name": "llm-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
