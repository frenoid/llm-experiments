{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6eecbd-f197-4550-a87e-4af74eee3218",
   "metadata": {},
   "source": [
    "# Using the CodeLlama-7B-GGUF for simple inference\n",
    "This notebook shows how to use the CodeLlama-7B-GGUF for basic inference\n",
    "It requires a GPU, I tested it on a GTX 4090 with 24 GB of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd505d-a593-45c6-bbef-06b393e362b1",
   "metadata": {},
   "source": [
    "Remove any previously downloaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7dda188-a703-4941-9058-f0643e1fb59e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! set +x && rm -rf model-cache/CodeLlama-7B-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a52ce5-0e08-4163-bce2-655c34168630",
   "metadata": {
    "tags": []
   },
   "source": [
    "Clone the model repo and examine the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9863d679-d73f-41b3-a924-df922811d49c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CodeLlama-7B-GGUF'...\n",
      "remote: Enumerating objects: 177, done.\u001b[K\n",
      "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
      "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
      "remote: Total 177 (delta 21), reused 0 (delta 0), pack-reused 122\u001b[K\n",
      "Receiving objects: 100% (177/177), 45.74 KiB | 45.74 MiB/s, done.\n",
      "Resolving deltas: 100% (73/73), done.\n"
     ]
    }
   ],
   "source": [
    "! cd model-cache && GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/TheBloke/CodeLlama-7B-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68ebd8e1-cde9-48b1-8eaf-b2250888c904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 96\n",
      "-rw-r--r-- 1 ubuntu ubuntu  4766 Apr 28 10:28 USE_POLICY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 22762 Apr 28 10:28 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu   112 Apr 28 10:28 Notice\n",
      "-rw-r--r-- 1 ubuntu ubuntu  7020 Apr 28 10:28 LICENSE.txt\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q4_K_S.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q4_K_M.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q4_0.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q3_K_S.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q3_K_M.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q3_K_L.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q2_K.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu    29 Apr 28 10:28 config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q8_0.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q6_K.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q5_K_S.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q5_K_M.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu   135 Apr 28 10:28 codellama-7b.Q5_0.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltr model-cache/CodeLlama-7B-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274a055-7512-474a-9d5d-4648680d3fb3",
   "metadata": {},
   "source": [
    "# Download the model\n",
    "Now download the artifact codellama-7b.Q4_K_M.gguf\n",
    "\n",
    "## Model aspects\n",
    "1. File: codellama-7b.Q4_K_M.gguf\t\n",
    "2. Quantization method: Q4_K_M\t\n",
    "3. Bits: 4\n",
    "4. File size: 4.08 GB\t\n",
    "5. Memory required: 6.58 GB\t\n",
    "6. Use case: medium, balanced quality - recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4376907b-cc5c-4447-9749-5c985bef1cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFS objects: 100% (1/1), 4.1 GB | 112 MB/s                          \r"
     ]
    }
   ],
   "source": [
    "! cd model-cache/CodeLlama-7B-GGUF && git-lfs pull --include=codellama-7b.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26568b9e-078b-4583-a983-b739be5522f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3985544\n",
      "-rw-r--r-- 1 ubuntu ubuntu       4766 Apr 28 10:28 USE_POLICY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu      22762 Apr 28 10:28 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu        112 Apr 28 10:28 Notice\n",
      "-rw-r--r-- 1 ubuntu ubuntu       7020 Apr 28 10:28 LICENSE.txt\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q4_K_S.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q4_0.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q3_K_S.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q3_K_M.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q3_K_L.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q2_K.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu         29 Apr 28 10:28 config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q8_0.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q6_K.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q5_K_S.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q5_K_M.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu        135 Apr 28 10:28 codellama-7b.Q5_0.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu 4081095360 Apr 28 10:29 codellama-7b.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltr model-cache/CodeLlama-7B-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d8aa9-98b3-4e2d-aa7e-88932d738606",
   "metadata": {},
   "source": [
    "# Model inferencing\n",
    "Now we will use the AutoModelForCausalLM method to load a pre-trained model and use it for inference\n",
    "\n",
    "We will need to point it to the directory where the model was downloaded and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f58e50-032d-40aa-b37d-13009424cc36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm-python3-11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " change the way we work and play\n",
      " In this book, we’ll show you how AI can make your life easier, faster, safer, more secure, smarter, better. And how it will also affect every aspect of your business – from marketing and sales to productivity and healthcare. You’ll see that the most successful companies are using AI to change their processes, streamline operations, increase revenue, improve profitability, reduce costs, and protect themselves against attack by both good guys and bad. This is going to be big stuff for companies to make right decisions with minimal human input (except the very highest levels) but great benefit to humans of every sort as they see new worlds become real, all the time!\n",
      " AI today isn’t quite a mirage but there is enough information here now about it so you will see more examples, real or virtual. Also the Internet-based versions (of human actions or speech) of everything described will get cheaper, quicker and smarter each day; especially by your home machines doing jobs formerly done on distant systems for the rest of your lifetime with human errors which no software has access to… just data you will put on. If anything like these real m\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=\"./model-cache/CodeLlama-7B-GGUF\",\n",
    "                                           model_file=\"codellama-7b.Q4_K_M.gguf\",\n",
    "                                           model_type=\"llama\",\n",
    "                                           gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2227c3-6b1f-4fa4-86da-96265ab1cf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and I will show you how to do it in C++.\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "```\n",
      "\n",
      "The equivalent code in C++ is below. I will also explain what each line of the code does as well.\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "using namespace std;\n",
      "\n",
      "int factorial(int);\n",
      "\n",
      "int main() {\n",
      "    int n = 0;\n",
      "    cout << \"Enter a number: \";\n",
      "    cin >> n;\n",
      "    cout << \"Factorial of \" << n << \" is \" << factorial(n) << endl;\n",
      "}\n",
      "\n",
      "int factorial(int n) {\n",
      "    if (n == 0) return 1;\n",
      "    else return n * factorial(n - 1);\n",
      "}\n",
      "```\n",
      "\n",
      "### Function Declaration and Definition\n",
      "\n",
      "In C++, the first line of code is always a function declaration. A function declaration tells the compiler that this function exists, but it doesn't tell the compiler what this function does. It just tells the compiler\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Give me a Python function to calculate factorials\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2e474-b090-4e76-a3f5-a3ee27a1e587",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleanup\n",
    "First free the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5743a1e9-651e-42c5-a921-705c925440c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4f35a-007b-42b9-910a-bf92facbc53a",
   "metadata": {},
   "source": [
    "Next remove the downloaded artifacts to free disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f25d0e-c5cf-42c7-9070-af4dfb78fc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf model-cache/CodeLlama-7B-GGUF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 for LLM",
   "language": "python",
   "name": "llm-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
