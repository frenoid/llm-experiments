{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6eecbd-f197-4550-a87e-4af74eee3218",
   "metadata": {},
   "source": [
    "# Using the Phi-3-mini-4k-instruct-gguf for simple inference\n",
    "This notebook shows how to use the Phi-3-mini-4k-instruct-gguf for basic inference\n",
    "\n",
    "It requires a GPU, I tested it on a GTX 4090 with 24 GB of RAM\n",
    "\n",
    "[HuggingFace Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a3cbc-3232-46f2-a88e-b118e9b2d53e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CPU specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "750023ba-9cc1-4f18-9080-a51f9f8a8043",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Address sizes:                      48 bits physical, 48 bits virtual\n",
      "Byte Order:                         Little Endian\n",
      "CPU(s):                             24\n",
      "On-line CPU(s) list:                0-23\n",
      "Vendor ID:                          AuthenticAMD\n",
      "Model name:                         AMD Ryzen 9 7900X 12-Core Processor\n",
      "CPU family:                         25\n",
      "Model:                              97\n"
     ]
    }
   ],
   "source": [
    "! lscpu | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d3318-87ea-4f83-8618-3bb9c8969351",
   "metadata": {},
   "source": [
    "## GPU specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f9c0a30-f6d8-49bf-9a8e-dbb3889398d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78e281f-2256-4c07-8e39-926fa4a26d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "model_shard_dir='/home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf'\n",
    "model_shard_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7dda188-a703-4941-9058-f0643e1fb59e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(model_shard_dir\n",
    "              , ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "243c4d70-394e-41d9-bed7-f06b772befb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b762644-0630-491e-86d4-984eea45d85d",
   "metadata": {},
   "source": [
    "# Clone the git repo\n",
    "First get the git repo but without cloning the LFS objects using `GIT_LFS_SKIP_SMUDGE=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9863d679-d73f-41b3-a924-df922811d49c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'model-cache/Phi-3-mini-4k-instruct-gguf'...\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (15/15), 10.58 KiB | 10.58 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf model-cache/Phi-3-mini-4k-instruct-gguf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f0907-72e5-4459-bf57-6c73daff6770",
   "metadata": {},
   "source": [
    "Examine the artifacts\n",
    "\n",
    "Notice the gguf file are only 136 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ebd8e1-cde9-48b1-8eaf-b2250888c904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48K\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.8K May 22 16:23 NOTICE.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  251 May 22 16:23 Modelfile_q4\n",
      "-rw-r--r-- 1 ubuntu ubuntu  253 May 22 16:23 Modelfile_fp16\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.1K May 22 16:23 LICENSE\n",
      "-rw-r--r-- 1 ubuntu ubuntu  444 May 22 16:23 CODE_OF_CONDUCT.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.6K May 22 16:23 SECURITY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  14K May 22 16:23 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  135 May 22 16:23 Phi-3-mini-4k-instruct-q4.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu  135 May 22 16:23 Phi-3-mini-4k-instruct-fp16.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/Phi-3-mini-4k-instruct-gguf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274a055-7512-474a-9d5d-4648680d3fb3",
   "metadata": {},
   "source": [
    "## Download the model Phi-3-mini-4k-instruct-q4.gguf\n",
    "First we test the __Phi-3-mini-4k-instruct-q4.gguf__ model\n",
    "\n",
    "The model card lists this as `medium, balanced quality - recommended`\n",
    "\n",
    "Now download the artifact Phi-3-mini-4k-instruct-q4.gguf\n",
    "\n",
    "### Model aspects\n",
    "1. File: Phi-3-mini-4k-instruct-q4.gguf\n",
    "2. Quantization method: Q4_K_M\t\n",
    "3. Bits: 4\n",
    "4. File size: 2.2GB\n",
    "5. Memory required: 4.7 GB\t\n",
    "6. Use case: medium, balanced quality - recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4376907b-cc5c-4447-9749-5c985bef1cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFS objects: 100% (1/1), 2.4 GB | 109 MB/s                          \r"
     ]
    }
   ],
   "source": [
    "! cd model-cache/Phi-3-mini-4k-instruct-gguf  && git-lfs pull --include=Phi-3-mini-4k-instruct-q4.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26568b9e-078b-4583-a983-b739be5522f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.3G\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.8K May 22 16:58 NOTICE.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  251 May 22 16:58 Modelfile_q4\n",
      "-rw-r--r-- 1 ubuntu ubuntu  253 May 22 16:58 Modelfile_fp16\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.1K May 22 16:58 LICENSE\n",
      "-rw-r--r-- 1 ubuntu ubuntu  444 May 22 16:58 CODE_OF_CONDUCT.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.6K May 22 16:58 SECURITY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  14K May 22 16:58 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  135 May 22 16:58 Phi-3-mini-4k-instruct-fp16.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.3G May 22 16:59 Phi-3-mini-4k-instruct-q4.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/Phi-3-mini-4k-instruct-gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d8aa9-98b3-4e2d-aa7e-88932d738606",
   "metadata": {},
   "source": [
    "## Model inferencing with Phi-3-mini-4k-instruct-q4.gguf\n",
    "`Phi-3-mini-4k-instruct-gguf` requires the `llama-cpp-python` to be installed\n",
    "\n",
    "You can do this with\n",
    "```bash\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f58e50-032d-40aa-b37d-13009424cc36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    52.84 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2228.82 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   300.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    14.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=f\"{model_shard_dir}/Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\n",
    "  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb67a43-6b99-4ed7-b415-b1d7c398de9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 17:00:46 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   40C    P2             59W /  450W |    4459MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     46017      C   ...onda/envs/llm-python3-11/bin/python       4452MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2227c3-6b1f-4fa4-86da-96265ab1cf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     130.02 ms\n",
      "llama_print_timings:      sample time =      25.53 ms /   256 runs   (    0.10 ms per token, 10029.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.57 ms /    12 tokens (   10.80 ms per token,    92.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1113.32 ms /   255 runs   (    4.37 ms per token,   229.05 tokens per second)\n",
      "llama_print_timings:       total time =    1524.88 ms /   267 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is an embedding model?<|end|>\n",
      "<|assistant|> An embedding model, in the context of machine learning and natural language processing (NLP), refers to a mathematical representation where words or phrases from the vocabulary are transformed into vectors of real numbers. Each unique word or phrase corresponds to a specific vector in a high-dimensional space, with each dimension capturing some aspect of the word's meaning based on its usage context within large corpora.\n",
      "\n",
      "\n",
      "This transformation allows machines to process and understand human language by leveraging similarities between words (such as synonyms) because they tend to be closer together in this vector space. Embeddings can also capture relationships, analogies, and other linguistic patterns. Common types of embedding models include:\n",
      "\n",
      "\n",
      "- Word2Vec: Uses neural networks to produce dense word vectors by predicting a context given a targeted word (continuous bag-of-words model) or vice versa.\n",
      "\n",
      "- GloVe (Global Vectors for Word Representation): An unsupervised learning algorithm that constructs vectors by analyzing co-occurrence statistics from a corpus of text.\n",
      "\n",
      "- FastText: Similar to Word2Vec but considers subword information, making it better at handling rare and out-of\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is an embedding model?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f9edf10-09fe-4647-b906-9cd7a5fed353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4017.01 ms\n",
      "llama_print_timings:      sample time =      24.10 ms /   256 runs   (    0.09 ms per token, 10621.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =      62.33 ms /    10 tokens (    6.23 ms per token,   160.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1104.35 ms /   255 runs   (    4.33 ms per token,   230.91 tokens per second)\n",
      "llama_print_timings:       total time =    1437.72 ms /   265 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What are closures in functional programming?<|end|>\n",
      "<|assistant|> Closures in functional programming refer to the concept where a function captures the lexical scope in which it was defined. This means that a closure can access variables from its outer (enclosing) scope, even after the outer functions have finished executing. This is an important feature because it allows for powerful and flexible abstractions, as well as encapsulation of state without relying on global variables or object properties.\n",
      "\n",
      "\n",
      "Here's a basic example to illustrate closures in JavaScript:\n",
      "\n",
      "```javascript\n",
      "function makeCounter() {\n",
      "    let count = 0; // This is the lexical scope accessible by the closure (the inner function).\n",
      "    \n",
      "    return function incrementCounter() {\n",
      "        // Closure capturing 'count' from its outer environment.\n",
      "        console.log(count);\n",
      "        count++;\n",
      "    };\n",
      "}\n",
      "\n",
      "const counter = makeCounter();  // The `incrementCounter` function retains access to the `count` variable defined in its surrounding scope.\n",
      "\n",
      "counter(); // Outputs: 0\n",
      "counter(); // Outputs: 1\n",
      "```\n",
      "\n",
      "In this example, the inner function `incrementCounter` forms a closure over the variable `count`. Each time `counter\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are closures in functional programming?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b74c4-961e-4960-a0a6-736e68ec37f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the model Phi-3-mini-4k-instruct-fp16.gguf\n",
    "Next we test the __Phi-3-mini-4k-instruct-fp16.gguf__ model\n",
    "\n",
    "The model card lists this as `minimal quality loss`\n",
    "\n",
    "Now download the artifact [Phi-3-mini-4k-instruct-q4.gguf](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-fp16.gguf)\n",
    "\n",
    "### Model aspects\n",
    "1. File: Phi-3-mini-4k-instruct-q4.gguf\n",
    "2. Quantization method: None\t\n",
    "3. Bits: NA\n",
    "4. File size: 7.2 GB\n",
    "5. Memory required: 9.3 GiB\n",
    "6. Use case: minimal quality loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9374667b-5426-4e2d-bd0f-2446bab9be4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFS objects: 100% (1/1), 7.6 GB | 108 MB/s                          \r"
     ]
    }
   ],
   "source": [
    "! cd model-cache/Phi-3-mini-4k-instruct-gguf  && git-lfs pull --include=Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2555392-cf29-4dbc-82ce-0f8515dd46e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.3G\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.8K May 22 16:23 NOTICE.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  251 May 22 16:23 Modelfile_q4\n",
      "-rw-r--r-- 1 ubuntu ubuntu  253 May 22 16:23 Modelfile_fp16\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.1K May 22 16:23 LICENSE\n",
      "-rw-r--r-- 1 ubuntu ubuntu  444 May 22 16:23 CODE_OF_CONDUCT.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.6K May 22 16:23 SECURITY.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  14K May 22 16:23 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu  135 May 22 16:23 Phi-3-mini-4k-instruct-fp16.gguf\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.3G May 22 16:23 Phi-3-mini-4k-instruct-q4.gguf\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/Phi-3-mini-4k-instruct-gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a8a39-554e-45d7-9e2f-5c96830c9752",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model inferencing with Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e9bb51-f295-4c07-8111-277d3a41486c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /home/ubuntu/jupyterhub/llms/model-cache/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  130 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   187.88 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  7100.64 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   300.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    14.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=f\"{model_shard_dir}/Phi-3-mini-4k-instruct-fp16.gguf\",  # path to GGUF file\n",
    "  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692dfdf7-27ff-461c-853c-5579741f03d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 16:53:25 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   38C    P8             25W /  450W |    9347MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     38857      C   ...onda/envs/llm-python3-11/bin/python       9340MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f0d3dbd-052a-49f6-b913-89b8e7bd8d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     136.41 ms\n",
      "llama_print_timings:      sample time =      24.90 ms /   256 runs   (    0.10 ms per token, 10282.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     136.31 ms /    12 tokens (   11.36 ms per token,    88.03 tokens per second)\n",
      "llama_print_timings:        eval time =    2456.00 ms /   255 runs   (    9.63 ms per token,   103.83 tokens per second)\n",
      "llama_print_timings:       total time =    2866.99 ms /   267 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is an embedding model?<|end|>\n",
      "<|assistant|> An embedding model is a mathematical representation used in machine learning and natural language processing (NLP) to convert large, sparse data sets into dense vectors of fixed size. These vectors are known as \"embeddings\" and they capture the essential properties or relationships within the original data by placing similar items close together in the vector space.\n",
      "\n",
      "Embedding models typically work on one-hot encoded categorical data (like words from a vocabulary) or discrete variables, transforming them into continuous vectors. This process enables complex computations that can reveal patterns and relationships between elements, which were not initially apparent due to their high-dimensional and sparse nature.\n",
      "\n",
      "In NLP, embedding models are essential for tasks such as:\n",
      "\n",
      "1. Word Representation: Words or phrases are represented by dense vectors (word embeddings), capturing semantic meanings based on contextual information from large corpora of text. Popular word embedding techniques include Word2Vec and GloVe.\n",
      "   Example: In the Skip-gram model, a window size is defined around each target word, with its surrounding words used to create the context vectors for training an embedding matrix.\n",
      "   \n",
      "2. Sentence or Document Embeddings: Ent\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is an embedding model?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e22cd9b-fec8-4ef4-acbc-d01523d88e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     136.41 ms\n",
      "llama_print_timings:      sample time =      24.34 ms /   256 runs   (    0.10 ms per token, 10518.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =      64.32 ms /    10 tokens (    6.43 ms per token,   155.46 tokens per second)\n",
      "llama_print_timings:        eval time =    2456.95 ms /   255 runs   (    9.64 ms per token,   103.79 tokens per second)\n",
      "llama_print_timings:       total time =    2796.77 ms /   265 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What are closures in functional programming?<|end|>\n",
      "<|assistant|> Closures are an important concept in functional programming, and they play a critical role in managing state and creating functions with encapsulated behavior. A closure is created when a function captures variables from its surrounding environment (lexical scope), effectively \"closing over\" these variables to form a new scope that exists even after the outer function has finished executing. This allows the inner function to remember and access those captured variables, regardless of where it's called later in your program.\n",
      "\n",
      "Here's an example using JavaScript to illustrate closures:\n",
      "\n",
      "```javascript\n",
      "function createCounter() {\n",
      "  let count = 0; // A variable defined within the outer scope (createCounter function)\n",
      "\n",
      "  return function () {\n",
      "    count += 1; // The inner function has access and can modify 'count' even though it is not directly accessible from outside\n",
      "    console.log(count);\n",
      "  };\n",
      "}\n",
      "\n",
      "const counter = createCounter();\n",
      "counter(); // Outputs: 1\n",
      "counter(); // Outputs: 2\n",
      "counter(); // Outputs: 3\n",
      "```\n",
      "\n",
      "In this example, `createCounter` is a function that returns another function (an inner function) as its result. The inner\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are closures in functional programming?\"\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=256,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"], \n",
    "  echo=True,  # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2e474-b090-4e76-a3f5-a3ee27a1e587",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Free the GPU memory\n",
    "Reset the cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5743a1e9-651e-42c5-a921-705c925440c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ce66ba-f279-4a5d-8e4b-26226a35d079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 17:01:23 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   40C    P5             27W /  450W |       5MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964f2c2-c87c-46fa-9d45-3938b7801c64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean up the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4f35a-007b-42b9-910a-bf92facbc53a",
   "metadata": {},
   "source": [
    "Next remove the downloaded artifacts to free disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f25d0e-c5cf-42c7-9070-af4dfb78fc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf model-cache/CodeLlama-7B-GGUF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 for LLM",
   "language": "python",
   "name": "llm-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
