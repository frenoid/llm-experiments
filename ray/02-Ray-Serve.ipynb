{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9210c46-914b-421d-aa8a-76488444d875",
   "metadata": {},
   "source": [
    "# Getting Started with Ray Serve\n",
    "Adapted from [Getting Started](https://docs.ray.io/en/latest/serve/getting_started.html)\n",
    "This tutorial will walk you through the process of writing and testing a Ray Serve application. It will show you how to\n",
    "\n",
    "* convert a machine learning model to a Ray Serve deployment\n",
    "* test a Ray Serve application locally over HTTP\n",
    "* compose multiple-model machine learning models together into a single application\n",
    "\n",
    "We’ll use two models in this tutorial:\n",
    "* [HuggingFace’s TranslationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TranslationPipeline) as a text-translation model\n",
    "* [HuggingFace’s SummarizationPipeline](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines#transformers.SummarizationPipeline) as a text-summarizer model\n",
    "\n",
    "After deploying those two models, we’ll test them with HTTP requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d61e2-a55c-4adc-8f57-9b676e533095",
   "metadata": {},
   "source": [
    "## Text Translation Model (before Ray Serve)\n",
    "First, we will download the [optimum/t5-small](https://huggingface.co/optimum/t5-small/tree/main) model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3df2340-537a-46c3-af1c-824d718d56db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 't5-small'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 100, done.\u001b[K\n",
      "remote: Total 100 (delta 0), reused 0 (delta 0), pack-reused 100 (from 1)\u001b[K\n",
      "Receiving objects: 100% (100/100), 970.79 KiB | 1.31 MiB/s, done.\n",
      "Resolving deltas: 100% (43/43), done.\n"
     ]
    }
   ],
   "source": [
    "! cd model-cache && GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/google-t5/t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d905e307-5d2e-4804-8545-bcdfe2184d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.2M\n",
      "-rw-r--r-- 1 ubuntu ubuntu 8.3K May 26 16:04 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.2K May 26 16:04 config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 rust_model.ot\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 pytorch_model.bin\n",
      "drwxr-xr-x 2 ubuntu ubuntu 4.0K May 26 16:04 onnx\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 model.safetensors\n",
      "-rw-r--r-- 1 ubuntu ubuntu  147 May 26 16:04 generation_config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 flax_model.msgpack\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 tf_model.h5\n",
      "-rw-r--r-- 1 ubuntu ubuntu 774K May 26 16:04 spiece.model\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.4M May 26 16:04 tokenizer.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.3K May 26 16:04 tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b106b42b-d830-43fb-bfeb-1c0a32c01608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFS objects:   0% (0/1), 0 B | 0 B/s                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFS objects: 100% (1/1), 242 MB | 13 MB/s                           \r"
     ]
    }
   ],
   "source": [
    "! cd model-cache/t5-small && \\\n",
    "git lfs pull --include=\"model.safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7847ba97-aea8-405f-8f24-442880542eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 233M\n",
      "-rw-r--r-- 1 ubuntu ubuntu 8.3K May 26 16:04 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.2K May 26 16:04 config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 rust_model.ot\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 pytorch_model.bin\n",
      "drwxr-xr-x 2 ubuntu ubuntu 4.0K May 26 16:04 onnx\n",
      "-rw-r--r-- 1 ubuntu ubuntu  147 May 26 16:04 generation_config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 flax_model.msgpack\n",
      "-rw-r--r-- 1 ubuntu ubuntu  134 May 26 16:04 tf_model.h5\n",
      "-rw-r--r-- 1 ubuntu ubuntu 774K May 26 16:04 spiece.model\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.4M May 26 16:04 tokenizer.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2.3K May 26 16:04 tokenizer_config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 231M May 26 16:05 model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh model-cache/t5-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078f537-767b-43b4-bfb3-e89515d94617",
   "metadata": {},
   "source": [
    "First, let’s take a look at our text-translation model. Here’s its code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9932b9b3-b90f-4f48-b418-127f0993c87e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: [{'translation_text': 'Bonjour monde!'}]\n",
      "Bonjour monde!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        self.model = pipeline(\"translation_en_to_fr\",\n",
    "                              model=\"./model-cache/t5-small\")\n",
    "\n",
    "    def translate(self, text: str) -> str:\n",
    "        # Run inference\n",
    "        model_output = self.model(text)\n",
    "        print(f\"Raw output: {model_output}\")\n",
    "\n",
    "        # Post-process output to return only the translation text\n",
    "        translation = model_output[0][\"translation_text\"]\n",
    "\n",
    "        return translation\n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "translation = translator.translate(\"Hello world!\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871de4f0-a70c-4741-9319-9670aa4172da",
   "metadata": {},
   "source": [
    "The Python file, called `model.py`, uses the `Translator` class to translate English text to French.\n",
    "\n",
    "The `self.model` variable inside `Translator`’s `__init__` method stores a function that uses the [t5-small model](https://huggingface.co/google-t5/t5-small/tree/main) to translate text.\n",
    "\n",
    "When `self.model` is called on English text, it returns translated French text inside a dictionary formatted as `[{\"translation_text\": \"...\"}]`.\n",
    "\n",
    "The `Translator`’s `translate` method extracts the translated text by indexing into the dictionary.\n",
    "\n",
    "You can copy-paste this script and run it locally. It translates `\"Hello world!\"` into `\"Bonjour Monde!\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93de43e-d3a1-416a-951b-27a9be5fe60e",
   "metadata": {},
   "source": [
    "Keep in mind that the `TranslationPipeline` is an example ML model for this tutorial. You can follow along using arbitrary models from any Python framework. Check out our tutorials on scikit-learn, PyTorch, and Tensorflow for more info and examples:\n",
    "\n",
    "* [Serve ML Models (Tensorflow, PyTorch, Scikit-Learn, others)](https://docs.ray.io/en/latest/serve/tutorials/serve-ml-models.html#serve-ml-models-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237e596-edea-41bd-8d00-3defb6b545ea",
   "metadata": {},
   "source": [
    "## Converting to a Ray Serve Application\n",
    "In this section, we’ll deploy the text translation model using Ray Serve, so it can be scaled up and queried over HTTP. We’ll start by converting Translator into a Ray Serve deployment.\n",
    "\n",
    "First, we open a new Python file and import `ray` and `ray.serve`:\n",
    "\n",
    "After these imports, we can include our model code from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8b6e5d-c789-4d2f-ac10-28de0abd543a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm-python3-11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-26 18:31:52,541\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from starlette.requests import Request\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=1,\n",
    "                  ray_actor_options={\"num_cpus\": 0.5, \"num_gpus\": 0})\n",
    "class Translator:\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        self.model = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "        print(\"Initialized!\")\n",
    "\n",
    "    def translate(self, text: str) -> str:\n",
    "        print(\"Translating ...\")\n",
    "              \n",
    "        # Run inference\n",
    "        model_output = self.model(text)\n",
    "\n",
    "        # Post-process output to return only the translation text\n",
    "        translation = model_output[0][\"translation_text\"]\n",
    "\n",
    "        return translation\n",
    "\n",
    "    async def __call__(self, http_request: Request) -> str:\n",
    "        english_text: str = await http_request.json()\n",
    "        return self.translate(english_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46d3cc-bbac-47ef-8b50-1d5b49b2b4ed",
   "metadata": {},
   "source": [
    "The `Translator` class has two modifications:\n",
    "\n",
    "1. It has a decorator, `@serve.deployment`.\n",
    "\n",
    "2. It has a new method, `__call__`.\n",
    "\n",
    "The decorator converts `Translator` from a Python class into a Ray Serve `Deployment` object.\n",
    "\n",
    "Each deployment stores a single Python function or class that you write and uses it to serve requests. You can scale and configure each of your deployments independently using parameters in the `@serve.deployment` decorator. The example configures a few common parameters:\n",
    "\n",
    "* `num_replicas`: an integer that determines how many copies of our deployment process run in Ray. Requests are load balanced across these replicas, allowing you to scale your deployments horizontally.\n",
    "\n",
    "* `ray_actor_options`: a dictionary containing configuration options for each replica.\n",
    "\n",
    "* `num_cpus`: a float representing the logical number of CPUs each replica should reserve. You can make this a fraction to pack multiple replicas together on a machine with fewer CPUs than replicas.\n",
    "\n",
    "* `num_gpus`: a float representing the logical number of GPUs each replica should reserve. You can make this a fraction to pack multiple replicas together on a machine with fewer GPUs than replicas.\n",
    "\n",
    "All these parameters are optional, so feel free to omit them\n",
    "\n",
    "Deployments receive Starlette `HTTP` request objects. By default, the deployment class’s `__call__` method is called on this request object. The return value is sent back in the HTTP response body.\n",
    "\n",
    "This is why `Translator` needs a new `__call__` method. The method processes the incoming HTTP request by reading its JSON data and forwarding it to the translate method. The translated text is returned and sent back through the HTTP response. You can also use Ray Serve’s FastAPI integration to avoid working with raw HTTP requests. Check out [FastAPI HTTP Deployments](https://docs.ray.io/en/latest/serve/http-guide.html#serve-fastapi-http) for more info about FastAPI with Serve.\n",
    "\n",
    "Next, we need to `bind` our `Translator` deployment to arguments that will be passed into its constructor. This defines a Ray Serve application that we can run locally or deploy to production (you’ll see later that applications can consist of multiple deployments). Since `Translator`’s constructor doesn’t take in any arguments, we can call the deployment’s `bind` method without passing anything in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62f617c-8fb2-4cc6-8392-d289c2a12af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator_app = Translator.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69161934-1dfe-4b7b-89ad-4490a97389b3",
   "metadata": {},
   "source": [
    "## Start the Ray Serve\n",
    "Start the Ray Serve proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52c7b74-b93a-4513-a235-2f8357aa7582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 18:32:04,954\tINFO worker.py:1429 -- Using address ray://localhost:10001 set in the environment variable RAY_ADDRESS\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n",
      "2024-05-26 18:32:05,686\tWARNING utils.py:1591 -- Python patch version mismatch: The cluster was started with:\n",
      "    Ray: 2.23.0\n",
      "    Python: 3.11.9\n",
      "This process on Ray Client was started with:\n",
      "    Ray: 2.23.0\n",
      "    Python: 3.11.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "runtime_env = {\n",
    "    \"pip\": [\n",
    "            \"ipython==8.12.3\",\n",
    "            \"transformers==4.38.2\"\n",
    "           ],\n",
    "    \"env_vars\": {\"TF_WARNINGS\": \"none\"}\n",
    "}\n",
    "\n",
    "\n",
    "os.environ[\"RAY_ADDRESS\"] = \"ray://localhost:10001\"\n",
    "\n",
    "def r_init():\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "r_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749ac9c8-8f9f-461f-9b05-565ca56001a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0e000000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.runtime_context.get_runtime_context().get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a178039-f205-4fca-8866-a97aed0b3dd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start the Ray Serve Proxy and Controller\n",
    "SSH into the Ray Cluster head node and execute this command\n",
    "```bash\n",
    "serve start --http-host=0.0.0.0 --proxy-location=HeadOnly\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda75176-3912-4eaf-a7a3-119c7c69e9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServeStatus(proxies={'02cb3c849ec98e7acfbbe84c6ad3c42854231dc30b814a9b6ba4ad08': <ProxyStatus.HEALTHY: 'HEALTHY'>}, applications={}, target_capacity=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db99e43-05bf-4565-adf2-0987f90ebfd9",
   "metadata": {},
   "source": [
    "## Deploy the translator_app application\n",
    "Use `ray.serve.run` to deploy the `translator_app` onto the Ray Cluster so that it becomes available at `http://<ray-cluster-host>:8000/translate`\n",
    "\n",
    "Reference API:\n",
    "* [ray.serve.run](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.run.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2245533a-f7f7-419f-9513-e7b85bd65b83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new client HTTP config differs from the existing one in the following fields: ['host', 'location']. The new HTTP config is ignored.\n",
      "2024-05-26 18:32:15,498\tINFO handle.py:126 -- Created DeploymentHandle 'sp6k5kt0' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:15,499\tINFO handle.py:126 -- Created DeploymentHandle '2bosd1c8' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:19,526\tINFO handle.py:126 -- Created DeploymentHandle 'g5pfku8c' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:19,527\tINFO api.py:584 -- Deployed app 'eng_to_fr' successfully.\n",
      "2024-05-26 18:32:31,538\tINFO handle.py:126 -- Created DeploymentHandle '53nf07ws' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,538\tINFO handle.py:126 -- Created DeploymentHandle '68pwf35w' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,539\tINFO handle.py:126 -- Created DeploymentHandle 'jcl61gjg' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,539\tINFO handle.py:126 -- Created DeploymentHandle '2rbxstfy' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,539\tINFO handle.py:126 -- Created DeploymentHandle 'p6bngts9' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,539\tINFO handle.py:126 -- Created DeploymentHandle 'i9y9d476' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,539\tINFO handle.py:126 -- Created DeploymentHandle 'zq1eot84' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,539\tINFO handle.py:126 -- Created DeploymentHandle 'zz6hg2hs' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,540\tINFO handle.py:126 -- Created DeploymentHandle 'xs5rxkl0' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,540\tINFO handle.py:126 -- Created DeploymentHandle 'vwrs0vmk' for Deployment(name='Translator', app='eng_to_fr').\n",
      "2024-05-26 18:32:31,540\tINFO handle.py:126 -- Created DeploymentHandle 'f4fw3e6j' for Deployment(name='Translator', app='eng_to_fr').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='Translator')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_name = \"eng_to_fr\"\n",
    "\n",
    "ray.serve.run(target=translator_app,\n",
    "              name=\"eng_to_fr\",\n",
    "              route_prefix=\"/translate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0f41c4-cb4c-44cf-92b6-2536afc017c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Un tronc d'arbre apricot donne un excellent bois\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "text = \"An apricot tree trunk yields excellent wood\"\n",
    "\n",
    "response = requests.post(\"http://ray-cluster-serve.mlnow.frenoid.com:30080/translate\", json=text)\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e097a95c-a03f-4cfb-83f0-09c4bcc719d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le fils de Michael me manque cher, ils doivent s'efforcer d'éviter de manquer le train.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "text = \"Michael's son misses me dearly. They must make haste to avoid missing the train.\"\n",
    "\n",
    "response = requests.post(\"http://ray-cluster-serve.mlnow.frenoid.com:30080/translate\", json=text)\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd09761-86c7-44b7-b15b-6e99ba5aa821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 18:38:39,779\tINFO pow_2_scheduler.py:260 -- Got updated replicas for Deployment(name='Translator', app='eng_to_fr'): set().\n"
     ]
    }
   ],
   "source": [
    "serve.delete(app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ba6c4-6e53-44f4-b66e-5769bb895f22",
   "metadata": {},
   "source": [
    "# Composing Multiple Models\n",
    "Ray Serve allows you to compose multiple deployments into a single Ray Serve application. This makes it easy to combine multiple machine learning models along with business logic to serve a single request. We can use parameters like `autoscaling_config`, `num_replicas`, `num_cpus`, and `num_gpus` to independently configure and scale each deployment in the application.\n",
    "\n",
    "For example, let’s deploy a machine learning pipeline with two steps:\n",
    "\n",
    "1. Summarize English text\n",
    "2. Translate the summary into French\n",
    "\n",
    "`Translator` already performs step 2. We can use [HuggingFace’s SummarizationPipeline](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines#transformers.SummarizationPipeline) to accomplish step 1. Here’s an example of the `SummarizationPipeline` that runs locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e3812-19ec-4575-b630-1725df069dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 for LLM",
   "language": "python",
   "name": "llm-python-3-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
